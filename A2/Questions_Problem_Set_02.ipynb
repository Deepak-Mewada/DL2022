{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvnm3YqXF1et"
   },
   "source": [
    "\n",
    "# Problem Set 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK-9wPs4F1e1"
   },
   "source": [
    "**Name : `[Type your name here]`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWcIfKAnF1e3"
   },
   "source": [
    "**Roll-No : `[Type your rollno here]`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8ITxsdeF1e4"
   },
   "source": [
    "**Dept : `[Type your dept here]`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-IFdWvrF1e5"
   },
   "source": [
    "**Mail-ID : `[Type your mail here]`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idj_n4-4F1e5"
   },
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://docs.anaconda.com/anaconda/install/). Then save this file to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/).\n",
    "\n",
    "Put your solution into boxes marked with **`[double click here to add a solution]`** and press Ctr+Enter to render text. None of the parts of this assignment require use of a machine with a GPU. You may complete the assignment using your local machine or you may use Google Colaboratory.\n",
    "\n",
    "We highly encourage students to put down their answers to theoretical questions into corresponding cells below. However, if one does not know LaTeX (and would find it too hard to learn it), he/she can write it in pen-and-paper format and submit the scanned pdf. Note that the solutions to the programming problems should be submitted in the ipynb file itself.\n",
    "\n",
    "Submission instructions: Please upload your completed solution file to [KGP Moodle](https://kgpmoodle.iitkgp.ac.in/moodle/login/index.php) by the due date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgHLWu4Iq0sH"
   },
   "source": [
    "### Problem 1: Gradient Descent Update Rule\n",
    "\n",
    "**Q1.1**: In learning neural networks, we typically minimize a loss function $\\mathcal{L}(w)$ with respect to the network parameters $w$. It is also important that we *regularize* the network to reduce overfitting. A simple and popular regularization strategy is to penalize some *norm* of $w$.\n",
    "\n",
    "Consider that we have $N$ examples $(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)$ such that $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{0, 1\\}, i = 1...N$. Also consider that we have at our disposal a single neuron. Let $w = [w_1, w_2, ..., w_d]^T$ be the weight vector and the output be given by $\\hat y_i = \\sigma(w.x_i)$. The loss function is given by: $\\sum_{i=1}^N l(y_i, \\hat y_i) + \\lambda \\|w\\|^2$ where $\\lambda$ is the weight of regularization. Derive the update rule for minimizing this loss using stochastic gradient descent with step size $\\eta$ when $l(y_i, \\hat y_i) = log_e(1 + exp(-y_i. \\hat y_i))$. In other words, at time $t+1$, express the new parameters $w_{t+1}$ in terms of the old parameters $w_t$. **[10 marks]**\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d8SHS4hq5UR"
   },
   "source": [
    "**`[double click here to add a solution]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrvD5wLZInKB"
   },
   "source": [
    "### Problem 2: Numerical Overflow and Underflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWVcuGP7CdMh"
   },
   "source": [
    "Run the cell below. Is the output the same as that you would expect? This is due to the condition which is called [numerical underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow). It is the condition that occurs when the true result of a floating point operation is smaller in magnitude than the smallest value representable as a normal floating point number in the target datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrtvkkddFCJG"
   },
   "outputs": [],
   "source": [
    "1e10+1e-10 == 1e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LSxFiuOD0H9"
   },
   "source": [
    "Run the cell below. The warning message must have the word 'overflow' in it. This condition that occurs when a calculation produces a result that is greater in magnitude than the largest value representable in the target datatype is called [numerical overflow](https://en.wikipedia.org/wiki/Integer_overflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwPzQe4TD1zZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.exp(1000) == np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRUSbKitFLgl"
   },
   "source": [
    "**Q2.1**: How do people deal with numerical overflow and underflow? Why have we implemented $\\text{softplus}(x) = \\log(1+\\exp(x))$ as shown in the cell below? **[5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVfDG7dfF1e6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softplus(x_, limit=5):\n",
    "    x = np.array(x_)\n",
    "    compute_real_mask = np.logical_and(-limit < x, x < limit)\n",
    "    return_same_mask = x >= limit\n",
    "    computed_real_part = np.log(1 + np.exp(x*compute_real_mask)*compute_real_mask)\n",
    "    returned_same_part = x*return_same_mask\n",
    "    return computed_real_part + returned_same_part\n",
    "\n",
    "def test_softplus():\n",
    "    x_arr = np.linspace(-200, 200)\n",
    "    softplus_true = np.log(1 + np.exp(x_arr))\n",
    "    softplus_stable = softplus(x_arr)\n",
    "    assert np.max(abs(softplus_true-softplus_stable)) < 1e-3\n",
    "    \n",
    "test_softplus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJrAwp1CF1e8"
   },
   "source": [
    "**`[double click here to add a solution]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcgYjUKKInKF"
   },
   "source": [
    "### Problem 3: Perceptron Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjlfmcp1InKI"
   },
   "source": [
    "**Q3.1**: The code below generates $n$ 2D data points according to the Gaussian distribution $X1 \\sim \\mathcal{N}([1,0],\\,I_{2\\times2})$ and assigns them label 1. It also generates another $n$ 2D data points according to the Gaussian distribution $X2 \\sim \\mathcal{N}([-1,0],\\,I_{2\\times2})$ and assigns them label -1.\n",
    "\n",
    "If perceptron learning algorithm is used to classify the data, will it converge? You may add a code cell to answer this question.   **[5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MA0OGWYInKI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def generate_data(n):\n",
    "    '''Generate synthetic data'''\n",
    "    \n",
    "    X1 = np.random.multivariate_normal([1,0], [[1,0],[0,1]], n)\n",
    "    X2 = np.random.multivariate_normal([-1,0], [[1,0],[0,1]], n)\n",
    "    X = np.vstack((X1, X2))\n",
    "\n",
    "    y1 = np.ones(n, dtype=int)\n",
    "    y2 = -np.ones(n, dtype=int)\n",
    "    y = np.concatenate((y1, y2))\n",
    "    \n",
    "    return X, y\n",
    "X_train, y_train = generate_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7292YPWIHMA"
   },
   "source": [
    "**`[double click here to add a solution]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icMuAi-Lr1CO"
   },
   "source": [
    "### Problem 4: Implementing Multilayer Perceptron and Backpropagation   **[30 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZFH3r5liLSz"
   },
   "source": [
    "In this problem, we will be using [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of handwritten digits for classification task. A data folder that will be useful for doing this problem has been provided to you along with this notebook.\n",
    "\n",
    "Let us load the data first (it has been done for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKO-c29diLS0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "## Load the training data from the data folder\n",
    "output_dim = 10\n",
    "x_train = np.load('./data/X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "x_train = x_train / 255.0\n",
    "x_train = x_train.T\n",
    "gt_indices = np.load('./data/y_train.npy')\n",
    "train_length = x_train.shape[1]\n",
    "\n",
    "#creating one hot vector representation\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "y_train = y_train.T\n",
    "print(\"Number of training examples: {:d}\".format(train_length))\n",
    "\n",
    "## Loading the test data\n",
    "x_test = np.load('./data/X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "x_test = x_test / 255.0\n",
    "x_test = x_test.T\n",
    "gt_indices = np.load('./data/y_test.npy')\n",
    "test_length = x_test.shape[1]\n",
    "\n",
    "#creating one hot vector representation\n",
    "y_test = np.zeros((test_length, output_dim))\n",
    "for i in range(test_length):\n",
    "    y_test[i,gt_indices[i]] = 1\n",
    "y_test = y_test.T\n",
    "print(\"Number of test examples: {:d}\".format(test_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgce6j2MiLS1"
   },
   "source": [
    "This problem considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:  \n",
    "\n",
    "A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_j$  \n",
    "specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$\n",
    "\n",
    "An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$.\n",
    "\n",
    "We will use the following notation for quantities in a network:\n",
    "- Inputs to the network are $x_1,..., x_d$\n",
    "- Number of layers is $L$\n",
    "- There are $m^l$ inputs to layer $l$\n",
    "- There are $n^l = m^{l+1}$ outputs from layer $l$\n",
    "- The weight matrix for layer $l$ is $W^l$, an $m^l \\times n^l$ matrix, and the bias vector (offset) is $W_0^l$, an $n^l \\times 1$ vector\n",
    "- The outputs of the linear module for layer $l$ are known as pre-activation values and denoted $z^l$\n",
    "- The activation function at layer $l$ is $f^l(\\cdot)$\n",
    "- Layer $l$ activations are $a^l = [f^l(z^l_1), \\ldots, f^l(z^l_{n^l})]^T$\n",
    "- The output of the network is the values $a^L = [f^L(z^L_1), \\ldots, f^L(z^L_{n^L})]^T$\n",
    "- Loss function $Loss(a,y)$ measures the loss of output values $a$ when the target is $y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRIPVRhviLS1"
   },
   "source": [
    "We'll use the modular implementation, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
    "\n",
    "```\n",
    "# build a 3-layer network\n",
    "net = Sequential([Linear(2,3), Tanh(),\n",
    "                  Linear(3,3), Tanh(),\n",
    "    \t          Linear(3,2), SoftMax()])\n",
    "# train the network on data and labels\n",
    "net.sgd(x_train, y_train)\n",
    "```\n",
    "Please fill in any unimplemented methods below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cY3OEZWRiLS1"
   },
   "source": [
    "## Linear Modules: ##\n",
    "Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z; ; it can also store its input or output vectors for use by other methods (e.g., for subsequent backpropagation).\n",
    "\n",
    "$Z = W^T A + W_0$\n",
    "\n",
    "Each linear module has a backward method that takes in a column vector dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial A} = \\frac{\\partial Z}{\\partial A} \\frac{\\partial Loss}{\\partial Z}$ and similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HENTHEIPiLS1"
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def sgd_step(self, lrate): \n",
    "        pass # For modules without weights\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A   # (m x b)  b is the batch size, which is 1 for SGD\n",
    "        return None  ## ! -- code required # (n x b)\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        ## ! -- code required\n",
    "        self.dLdW = None       # (m x n)\n",
    "        self.dLdW0 = None      # (n x 1)\n",
    "        return None            # return dLdA (m x b)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        ## ! -- code required\n",
    "        self.W = None           \n",
    "        self.W0 = None          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_na1Jr-siLS2"
   },
   "source": [
    "## Activation functions: ##\n",
    "Activation modules don't have any weights and so they are simpler.\n",
    "\n",
    "Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n",
    "\n",
    "Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial Z} = \\frac{\\partial Loss}{\\partial A} \\frac{\\partial A}{\\partial Z}$\n",
    "\n",
    "For Softmax = $SM(Z)$ at the output layer and cross entropy as the $Loss(A,Y)$ function, there is a [simple form](https://peterroelants.github.io/posts/cross-entropy-softmax/) for ${\\tt dLdZ} = \\frac{\\partial Loss}{\\partial Z}$; namely, it is the prediction error $A−Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bY3HFGcHCkj"
   },
   "source": [
    "### Tanh: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFD9yB38iLS3"
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):            # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # Uses stored self.A\n",
    "        return None  ## ! -- code required # return dLdZ (?, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkBAcTbyiLS3"
   },
   "source": [
    "### ReLU: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNZgbM1ViLS3"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):              # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = None  ## ! -- code required # (?, b)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # uses stored self.A\n",
    "        return None  ## ! -- code required # return dLdZ (?, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qVlW8SFiLS3"
   },
   "source": [
    "### SoftMax: ###\n",
    "For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KrMWcgwiLS4"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):           # Output activation\n",
    "    def forward(self, Z):\n",
    "        return None  ## ! -- code required # (?, b)\n",
    "\n",
    "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        return None  ## ! -- code required # (1, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzwrK1XsiLS4"
   },
   "source": [
    "## Loss Function:##\n",
    "Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value.\n",
    "\n",
    "The CrossE module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with Cross Entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTp5-_H6iLS5"
   },
   "source": [
    "### Cross Entropy: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3t9-M8diLS5"
   },
   "outputs": [],
   "source": [
    "class CROSSE(Module):       # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        return None  ## ! -- code required\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        return None  ## ! -- code required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XrtkyTNiLS5"
   },
   "source": [
    "## Neural Network: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zFNsXNmiLS6"
   },
   "source": [
    "Implement SGD. Randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. Compute the predicted output Ypred for Xt with the forward method. Compute the loss for Ypred relative to Yt. Use the backward method to compute the gradients. Use the sgd_step method to change the weights. Repeat.\n",
    "\n",
    "Also, record the training accuracy after every 1000 iterations and plot it to show how the training accuracy changes with the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSPrY1tViLS6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, modules, loss):            # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        accuracies = [] # for plotting\n",
    "        itrns = [] # for plotting\n",
    "        for it in range(iters):\n",
    "            ## ! -- code required\n",
    "            if it % 1000 == 0 or it == iters-1:\n",
    "                acc = self.get_accuracy(X, Y)\n",
    "                accuracies.append(acc * 100)\n",
    "                itrns.append(it)\n",
    "                print('Iteration =', it, '\\tTraining Accuracy = %.2f%%' % (acc * 100))\n",
    "        ## ! -- code required\n",
    "        # plot accuracy vs iteration with appropriate labelling\n",
    "\n",
    "    def forward(self, Xt):                        # Compute Ypred\n",
    "        for m in self.modules: \n",
    "            Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: \n",
    "            delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):                    # Gradient descent step\n",
    "        for m in self.modules: \n",
    "            m.sgd_step(lrate)\n",
    "\n",
    "    def get_accuracy(self, X, Y):\n",
    "        # Method to print accuracy\n",
    "        cf = self.modules[-1].class_fun\n",
    "        acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OHGyeyJd1fu"
   },
   "source": [
    "Now, keeping the **number of hidden layers fixed at 2** and **learning rate fixed at 0.005** and the **number of iterations fixed at 17000**, try tuning the number of hidden units in each hidden layer as well as the activation function after every linear module. One example of a network having 30 hidden units in the first hidden layer followed by RELU activation and 20 in the second followed by Tanh activation is `nn = Sequential([Linear(input_dim, 30), ReLU(), Linear(30, 20), Tanh(), Linear(20,output_dim), SoftMax()], CROSSE())` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fpO_iwyiLS6"
   },
   "outputs": [],
   "source": [
    "input_dim = 784  # input dimension\n",
    "nn = None  ## ! -- code required\n",
    "nn.sgd(x_train, y_train, iters=17000, lrate=0.005)\n",
    "test_acc = nn.get_accuracy(x_test, y_test)\n",
    "print('Test Acc =', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmJ4TaVBF1fL"
   },
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "Wow, you made it this far. That is impressive. Congratulations!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Questions_Problem_Set_02.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
